version: '3.8'

networks:
  llm-network:
    driver: bridge

services:
  # ------------------------------------------------------------
  # 1. Nginx (The Gatekeeper)
  # ------------------------------------------------------------
  nginx:
    image: nginx:alpine
    container_name: llm-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      # Mount the entire LetsEncrypt folder for the certs
      - /etc/letsencrypt:/etc/letsencrypt:ro
      # Shared Challenge Folder
      - ./certbot/www:/var/www/certbot:ro
    depends_on:
      - ollama
    networks:
      - llm-network

  # ------------------------------------------------------------
  # 2. Certbot (The Renewer)
  # ------------------------------------------------------------
  certbot:
    image: certbot/certbot
    container_name: llm-certbot
    volumes:
      # Mount Host Certs (Read/Write)
      - /etc/letsencrypt:/etc/letsencrypt
      - /var/lib/letsencrypt:/var/lib/letsencrypt
      - ./certbot/www:/var/www/certbot
    # Renewal loop
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $!; done;'"
    networks:
      - llm-network

  # ------------------------------------------------------------
  # 3. Ollama (The AI Engine)
  # ------------------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    container_name: llm-ollama
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS="*"
      - OLLAMA_KEEP_ALIVE=24h
      - NVIDIA_VISIBLE_DEVICES=all
    networks:
      - llm-network

  # ------------------------------------------------------------
  # 4. Open WebUI (The Chat Interface)
  # ------------------------------------------------------------
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: llm-open-webui
    restart: unless-stopped
    volumes:
      - ./open-webui-data:/app/backend/data
    environment:
      # Use the container_name of your ollama service
      - OLLAMA_BASE_URL=http://llm-ollama:11434
      - WEBUI_AUTH=false
      - WEBUI_NAME="Open WebUI Chat AI"
    networks:
      - llm-network
